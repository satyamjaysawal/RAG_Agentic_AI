{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 4A. IMPORT KEY LIBRARIES & DEFINE KEY FUNCTIONS THAT WILL BE USED IN THE MCP SERVER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For this project, we will be using the `gradio` library to build the MCP server.\n",
    "- We will be using the `openai` library to interact with the OpenAI API.\n",
    "- We will be using the `dotenv` library to load the environment variables.\n",
    "\n",
    "\n",
    "\n",
    "What it does:\n",
    "\n",
    "- It is a server that exposes four powerful learning-oriented tools via the Model-Context-Protocol (MCP).\n",
    "- All functions are OpenAI-powered and most stream partial tokens for low-latency UX.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio[mcp] in c:\\users\\ryana\\anaconda3\\lib\\site-packages (5.29.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (4.9.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.115.9)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.10.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (1.10.0)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.30.2)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (3.10.18)\n",
      "Requirement already satisfied: packaging in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (10.4.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (2.11.4)\n",
      "Requirement already satisfied: pydub in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.11.8)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.45.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.15.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (4.13.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.34.2)\n",
      "Requirement already satisfied: mcp<2.0.0,>=1.6.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (1.8.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio-client==1.10.0->gradio[mcp]) (2025.3.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio-client==1.10.0->gradio[mcp]) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio[mcp]) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio[mcp]) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio[mcp]) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio[mcp]) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio[mcp]) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio[mcp]) (3.18.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio[mcp]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio[mcp]) (4.67.1)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from mcp<2.0.0,>=1.6.0->gradio[mcp]) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from mcp<2.0.0,>=1.6.0->gradio[mcp]) (2.9.1)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from mcp<2.0.0,>=1.6.0->gradio[mcp]) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio[mcp]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio[mcp]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio[mcp]) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio[mcp]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio[mcp]) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio[mcp]) (0.4.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio[mcp]) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio[mcp]) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio[mcp]) (13.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio[mcp]) (0.4.6)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pydantic-settings>=2.5.2->mcp<2.0.0,>=1.6.0->gradio[mcp]) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio[mcp]) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio[mcp]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio[mcp]) (2.15.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio[mcp]) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio[mcp]) (2.4.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio[mcp]) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the packages\n",
    "!pip install \"gradio[mcp]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutor_mcp_server.py\n",
    "\"\"\"AI Tutor MCP Toolkit\n",
    "======================\n",
    "\n",
    "A compact server that exposes **four** powerful learning‑oriented tools via the\n",
    "Model‑Context‑Protocol (MCP). All functions are OpenAI‑powered, and most stream\n",
    "partial tokens for low‑latency UX.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import io\n",
    "from typing import Generator, List\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Environment & OpenAI client setup\n",
    "# -----------------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file. Server cannot start.\")\n",
    "\n",
    "client = OpenAI(api_key = openai_api_key)\n",
    "MODEL_NAME = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Available MCP tools\n",
    "-------------------\n",
    "1. **`explain_concept`** – Stream an explanation of any concept at a chosen\n",
    "   complexity level (1 = kid‑friendly … 5 = expert).\n",
    "2. **`summarize_text`** – Stream a concise summary of long text, controllable\n",
    "   with a *compression_ratio*.\n",
    "3. **`generate_flashcards`** – Produce study flashcards (Q/A pairs) for rapid\n",
    "   review of a topic.\n",
    "4. **`quiz_me`** – Stream an interactive quiz on a topic; reveals answers after\n",
    "   questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the mapping from integers (1-5) to explanation levels\n",
    "EXPLANATION_LEVELS = {\n",
    "    1: \"like I'm 5 years old\",\n",
    "    2: \"like I'm 10 years old\",\n",
    "    3: \"like a high school student\",\n",
    "    4: \"like a college student\",\n",
    "    5: \"like an expert in the field\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the MCP Server.ipynb file, update the docstring as shown below\n",
    "\n",
    "def explain_concept(question: str, level: int) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream an explanation of *question* at the requested *level* (1‑5). If 1, explanation would be like we are talking to a 5 year old and if 5, explanation would be technical and complex.\"\"\"\n",
    "    if not question.strip():\n",
    "        yield \"Error: question cannot be blank.\"\n",
    "        return\n",
    "\n",
    "    level_desc = EXPLANATION_LEVELS.get(level, \"clearly and concisely\")\n",
    "    system_prompt = \"You are a helpful AI Tutor. Explain the following concept \" f\"{level_desc}.\"\n",
    "    _stream = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        stream=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    partial = \"\"\n",
    "    for chunk in _stream:\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if delta:\n",
    "            partial += delta\n",
    "            yield partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the explain concept function\n",
    "def explain_concept(question: str, level: int) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream an explanation of *question* at the requested *level* (1‑5).\"\"\"\n",
    "    if not question.strip():\n",
    "        yield \"Error: question cannot be blank.\"\n",
    "        return\n",
    "\n",
    "    level_desc = EXPLANATION_LEVELS.get(level, \"clearly and concisely\")\n",
    "    system_prompt = \"You are a helpful AI Tutor. Explain the following concept \" f\"{level_desc}.\"\n",
    "    _stream = client.chat.completions.create(\n",
    "        model = MODEL_NAME,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        stream = True,\n",
    "        temperature = 0.7,\n",
    "    )\n",
    "    partial = \"\"\n",
    "    for chunk in _stream:\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if delta:\n",
    "            partial += delta\n",
    "            yield partial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the summarize text function\n",
    "def summarize_text(text: str, compression_ratio: float = 0.3) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream a summary of *text* compressed to roughly *compression_ratio* length.\n",
    "\n",
    "    *compression_ratio* should be between 0.1 and 0.8.\n",
    "    \"\"\"\n",
    "    if not text.strip():\n",
    "        yield \"Error: text cannot be blank.\"\n",
    "        return\n",
    "    ratio = max(0.1, min(compression_ratio, 0.8))\n",
    "    system_prompt = (\n",
    "        \"You are a world‑class summarizer. Reduce the following text to about \"\n",
    "        f\"{int(ratio*100)}% of its original length while preserving key ideas.\"\n",
    "    )\n",
    "    _stream = client.chat.completions.create(\n",
    "        model = MODEL_NAME,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        stream = True,\n",
    "        temperature = 0.5,\n",
    "    )\n",
    "    partial = \"\"\n",
    "    for chunk in _stream:\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if delta:\n",
    "            partial += delta\n",
    "            yield partial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generate flashcards function\n",
    "def generate_flashcards(topic: str, num_cards: int = 5) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream *num_cards* Q/A flashcards for *topic* in JSON lines format.\"\"\"\n",
    "    if num_cards < 1 or num_cards > 20:\n",
    "        yield \"Error: num_cards must be between 1 and 20.\"\n",
    "        return\n",
    "    if not topic.strip():\n",
    "        yield \"Error: topic cannot be blank.\"\n",
    "        return\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an AI that generates study flashcards. \"\n",
    "        'Return each flashcard on its own line as JSON: {\"q\": <question>, \"a\": <answer>}'\n",
    "    )\n",
    "    user_prompt = f\"Create {num_cards} flashcards about {topic}.\"\n",
    "\n",
    "    _stream = client.chat.completions.create(\n",
    "        model = MODEL_NAME,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        stream = True,\n",
    "        temperature = 0.8,\n",
    "    )\n",
    "    partial = \"\"\n",
    "    for chunk in _stream:\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if delta:\n",
    "            partial += delta\n",
    "            yield partial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Quiz Me function\n",
    "\n",
    "def quiz_me(topic: str, level: int = 3, num_questions: int = 5) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream a quiz with numbered Qs then reveal answers after all questions.\"\"\"\n",
    "    if num_questions < 1 or num_questions > 15:\n",
    "        yield \"Error: num_questions must be between 1 and 15.\"\n",
    "        return\n",
    "    if not topic.strip():\n",
    "        yield \"Error: topic cannot be blank.\"\n",
    "        return\n",
    "\n",
    "    level_desc = EXPLANATION_LEVELS.get(level, \"at an intermediate level\")\n",
    "    system_prompt = (\n",
    "        \"You are an AI quiz master. Generate a quiz of multiple‑choice questions \"\n",
    "        f\"about {topic} {level_desc}. Number the questions. After listing all Qs, \"\n",
    "        \"add an \\nANSWER KEY section with the correct options.\"\n",
    "    )\n",
    "\n",
    "    _stream = client.chat.completions.create(\n",
    "        model = MODEL_NAME,\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}],\n",
    "        stream = True,\n",
    "        temperature = 0.7,\n",
    "    )\n",
    "    partial = \"\"\n",
    "    for chunk in _stream:\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if delta:\n",
    "            partial += delta\n",
    "            yield partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 4B. LAUNCH THE GRADIO MCP SERVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AI Tutor MCP Toolkit on port 7860…\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "\n",
      "🔨 MCP server (using SSE) running at: http://localhost:7860/gradio_api/mcp/sse\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def build_demo():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# AI Tutor MCP Toolkit – Demo Console\")\n",
    "        with gr.Tab(\"Explain Concept\"):\n",
    "            q = gr.Textbox(label=\"Concept / Question\")\n",
    "            lvl = gr.Slider(1, 5, value=3, step=1, label=\"Explanation Level\")\n",
    "            out1 = gr.Markdown()\n",
    "            gr.Button(\"Explain\").click(explain_concept, inputs=[q, lvl], outputs=out1)\n",
    "        with gr.Tab(\"Summarize Text\"):\n",
    "            txt = gr.Textbox(lines=8, label=\"Long Text\")\n",
    "            ratio = gr.Slider(0.1, 0.8, value=0.3, step=0.05, label=\"Compression Ratio\")\n",
    "            out2 = gr.Markdown()\n",
    "            gr.Button(\"Summarize\").click(summarize_text, inputs=[txt, ratio], outputs=out2)\n",
    "        with gr.Tab(\"Flashcards\"):\n",
    "            topic_fc = gr.Textbox(label=\"Topic\")\n",
    "            n_fc = gr.Slider(1, 20, value=5, step=1, label=\"# Cards\")\n",
    "            out3 = gr.Markdown()\n",
    "            gr.Button(\"Generate\").click(generate_flashcards, inputs=[topic_fc, n_fc], outputs=out3)\n",
    "        with gr.Tab(\"Quiz Me\"):\n",
    "            topic_q = gr.Textbox(label=\"Topic\")\n",
    "            lvl_q = gr.Slider(1, 5, value=3, step=1, label=\"Difficulty Level\")\n",
    "            n_q = gr.Slider(1, 15, value=5, step=1, label=\"# Questions\")\n",
    "            out4 = gr.Markdown()\n",
    "            gr.Button(\"Start Quiz\").click(quiz_me, inputs=[topic_q, lvl_q, n_q], outputs=out4)\n",
    "    return demo\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting AI Tutor MCP Toolkit on port 7860…\")\n",
    "    build_demo().launch(server_name = \"0.0.0.0\", mcp_server = True)\n",
    "# --- END OF FILE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
