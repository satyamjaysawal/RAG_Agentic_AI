{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 4A. IMPORT KEY LIBRARIES & DEFINE KEY FUNCTIONS THAT WILL BE USED IN THE MCP SERVER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For this project, we will be using the `gradio` library to build the MCP server.\n",
    "- We will be using the `openai` library to interact with the OpenAI API.\n",
    "- We will be using the `dotenv` library to load the environment variables.\n",
    "\n",
    "\n",
    "\n",
    "What it does:\n",
    "\n",
    "- It is a server that exposes four powerful learning-oriented tools via the Model-Context-Protocol (MCP).\n",
    "- All functions are OpenAI-powered and most stream partial tokens for low-latency UX.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio[mcp] in c:\\users\\ryana\\anaconda3\\lib\\site-packages (5.29.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (4.9.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.115.9)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.10.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (1.10.0)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.30.2)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (3.10.18)\n",
      "Requirement already satisfied: packaging in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (10.4.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (2.11.4)\n",
      "Requirement already satisfied: pydub in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.11.8)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.45.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.15.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (4.13.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (0.34.2)\n",
      "Requirement already satisfied: mcp<2.0.0,>=1.6.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio[mcp]) (1.8.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio-client==1.10.0->gradio[mcp]) (2025.3.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from gradio-client==1.10.0->gradio[mcp]) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio[mcp]) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio[mcp]) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio[mcp]) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio[mcp]) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio[mcp]) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio[mcp]) (3.18.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio[mcp]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio[mcp]) (4.67.1)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from mcp<2.0.0,>=1.6.0->gradio[mcp]) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from mcp<2.0.0,>=1.6.0->gradio[mcp]) (2.9.1)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from mcp<2.0.0,>=1.6.0->gradio[mcp]) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio[mcp]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio[mcp]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio[mcp]) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio[mcp]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio[mcp]) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio[mcp]) (0.4.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio[mcp]) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio[mcp]) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio[mcp]) (13.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio[mcp]) (0.4.6)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from pydantic-settings>=2.5.2->mcp<2.0.0,>=1.6.0->gradio[mcp]) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio[mcp]) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio[mcp]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio[mcp]) (2.15.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio[mcp]) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio[mcp]) (2.4.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ryana\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio[mcp]) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the packages\n",
    "!pip install \"gradio[mcp]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutor_mcp_server.py\n",
    "\"\"\"AI Tutor MCP Toolkit\n",
    "======================\n",
    "\n",
    "A compact server that exposes **four** powerful learningâ€‘oriented tools via the\n",
    "Modelâ€‘Contextâ€‘Protocol (MCP). All functions are OpenAIâ€‘powered, and most stream\n",
    "partial tokens for lowâ€‘latency UX.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import io\n",
    "from typing import Generator, List\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Environment & OpenAI client setup\n",
    "# -----------------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file. Server cannot start.\")\n",
    "\n",
    "client = OpenAI(api_key = openai_api_key)\n",
    "MODEL_NAME = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Available MCP tools\n",
    "-------------------\n",
    "1. **`explain_concept`** â€“ Stream an explanation of any concept at a chosen\n",
    "   complexity level (1Â = kidâ€‘friendly â€¦ 5Â = expert).\n",
    "2. **`summarize_text`** â€“ Stream a concise summary of long text, controllable\n",
    "   with a *compression_ratio*.\n",
    "3. **`generate_flashcards`** â€“ Produce study flashcards (Q/A pairs) for rapid\n",
    "   review of a topic.\n",
    "4. **`quiz_me`** â€“ Stream an interactive quiz on a topic; reveals answers after\n",
    "   questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the mapping from integers (1-5) to explanation levels\n",
    "EXPLANATION_LEVELS = {\n",
    "    1: \"like I'm 5 years old\",\n",
    "    2: \"like I'm 10 years old\",\n",
    "    3: \"like a high school student\",\n",
    "    4: \"like a college student\",\n",
    "    5: \"like an expert in the field\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the MCP Server.ipynb file, update the docstring as shown below\n",
    "\n",
    "def explain_concept(question: str, level: int) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream an explanation of *question* at the requested *level* (1â€‘5). If 1, explanation would be like we are talking to a 5 year old and if 5, explanation would be technical and complex.\"\"\"\n",
    "    if not question.strip():\n",
    "        yield \"Error: question cannot be blank.\"\n",
    "        return\n",
    "\n",
    "    level_desc = EXPLANATION_LEVELS.get(level, \"clearly and concisely\")\n",
    "    system_prompt = \"You are a helpful AI Tutor. Explain the following concept \" f\"{level_desc}.\"\n",
    "    _stream = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        stream=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    partial = \"\"\n",
    "    for chunk in _stream:\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if delta:\n",
    "            partial += delta\n",
    "            yield partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the explain concept function\n",
    "def explain_concept(question: str, level: int) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream an explanation of *question* at the requested *level* (1â€‘5).\"\"\"\n",
    "    if not question.strip():\n",
    "        yield \"Error: question cannot be blank.\"\n",
    "        return\n",
    "\n",
    "    level_desc = EXPLANATION_LEVELS.get(level, \"clearly and concisely\")\n",
    "    system_prompt = \"You are a helpful AI Tutor. Explain the following concept \" f\"{level_desc}.\"\n",
    "    _stream = client.chat.completions.create(\n",
    "        model = MODEL_NAME,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        stream = True,\n",
    "        temperature = 0.7,\n",
    "    )\n",
    "    partial = \"\"\n",
    "    for chunk in _stream:\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if delta:\n",
    "            partial += delta\n",
    "            yield partial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the summarize text function\n",
    "def summarize_text(text: str, compression_ratio: float = 0.3) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream a summary of *text* compressed to roughly *compression_ratio* length.\n",
    "\n",
    "    *compression_ratio* should be between 0.1 and 0.8.\n",
    "    \"\"\"\n",
    "    if not text.strip():\n",
    "        yield \"Error: text cannot be blank.\"\n",
    "        return\n",
    "    ratio = max(0.1, min(compression_ratio, 0.8))\n",
    "    system_prompt = (\n",
    "        \"You are a worldâ€‘class summarizer. Reduce the following text to about \"\n",
    "        f\"{int(ratio*100)}% of its original length while preserving key ideas.\"\n",
    "    )\n",
    "    _stream = client.chat.completions.create(\n",
    "        model = MODEL_NAME,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        stream = True,\n",
    "        temperature = 0.5,\n",
    "    )\n",
    "    partial = \"\"\n",
    "    for chunk in _stream:\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if delta:\n",
    "            partial += delta\n",
    "            yield partial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generate flashcards function\n",
    "def generate_flashcards(topic: str, num_cards: int = 5) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream *num_cards* Q/A flashcards for *topic* in JSON lines format.\"\"\"\n",
    "    if num_cards < 1 or num_cards > 20:\n",
    "        yield \"Error: num_cards must be between 1 and 20.\"\n",
    "        return\n",
    "    if not topic.strip():\n",
    "        yield \"Error: topic cannot be blank.\"\n",
    "        return\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an AI that generates study flashcards. \"\n",
    "        'Return each flashcard on its own line as JSON: {\"q\": <question>, \"a\": <answer>}'\n",
    "    )\n",
    "    user_prompt = f\"Create {num_cards} flashcards about {topic}.\"\n",
    "\n",
    "    _stream = client.chat.completions.create(\n",
    "        model = MODEL_NAME,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        stream = True,\n",
    "        temperature = 0.8,\n",
    "    )\n",
    "    partial = \"\"\n",
    "    for chunk in _stream:\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if delta:\n",
    "            partial += delta\n",
    "            yield partial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Quiz Me function\n",
    "\n",
    "def quiz_me(topic: str, level: int = 3, num_questions: int = 5) -> Generator[str, None, None]:\n",
    "    \"\"\"Stream a quiz with numbered Qs then reveal answers after all questions.\"\"\"\n",
    "    if num_questions < 1 or num_questions > 15:\n",
    "        yield \"Error: num_questions must be between 1 and 15.\"\n",
    "        return\n",
    "    if not topic.strip():\n",
    "        yield \"Error: topic cannot be blank.\"\n",
    "        return\n",
    "\n",
    "    level_desc = EXPLANATION_LEVELS.get(level, \"at an intermediate level\")\n",
    "    system_prompt = (\n",
    "        \"You are an AI quiz master. Generate a quiz of multipleâ€‘choice questions \"\n",
    "        f\"about {topic} {level_desc}. Number the questions. After listing all Qs, \"\n",
    "        \"add an \\nANSWER KEY section with the correct options.\"\n",
    "    )\n",
    "\n",
    "    _stream = client.chat.completions.create(\n",
    "        model = MODEL_NAME,\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}],\n",
    "        stream = True,\n",
    "        temperature = 0.7,\n",
    "    )\n",
    "    partial = \"\"\n",
    "    for chunk in _stream:\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if delta:\n",
    "            partial += delta\n",
    "            yield partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 4B. LAUNCH THE GRADIO MCP SERVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AI Tutor MCP Toolkit on port 7860â€¦\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "\n",
      "ðŸ”¨ MCP server (using SSE) running at: http://localhost:7860/gradio_api/mcp/sse\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def build_demo():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# AI Tutor MCP Toolkit â€“ Demo Console\")\n",
    "        with gr.Tab(\"Explain Concept\"):\n",
    "            q = gr.Textbox(label=\"Concept / Question\")\n",
    "            lvl = gr.Slider(1, 5, value=3, step=1, label=\"Explanation Level\")\n",
    "            out1 = gr.Markdown()\n",
    "            gr.Button(\"Explain\").click(explain_concept, inputs=[q, lvl], outputs=out1)\n",
    "        with gr.Tab(\"Summarize Text\"):\n",
    "            txt = gr.Textbox(lines=8, label=\"Long Text\")\n",
    "            ratio = gr.Slider(0.1, 0.8, value=0.3, step=0.05, label=\"Compression Ratio\")\n",
    "            out2 = gr.Markdown()\n",
    "            gr.Button(\"Summarize\").click(summarize_text, inputs=[txt, ratio], outputs=out2)\n",
    "        with gr.Tab(\"Flashcards\"):\n",
    "            topic_fc = gr.Textbox(label=\"Topic\")\n",
    "            n_fc = gr.Slider(1, 20, value=5, step=1, label=\"# Cards\")\n",
    "            out3 = gr.Markdown()\n",
    "            gr.Button(\"Generate\").click(generate_flashcards, inputs=[topic_fc, n_fc], outputs=out3)\n",
    "        with gr.Tab(\"Quiz Me\"):\n",
    "            topic_q = gr.Textbox(label=\"Topic\")\n",
    "            lvl_q = gr.Slider(1, 5, value=3, step=1, label=\"Difficulty Level\")\n",
    "            n_q = gr.Slider(1, 15, value=5, step=1, label=\"# Questions\")\n",
    "            out4 = gr.Markdown()\n",
    "            gr.Button(\"Start Quiz\").click(quiz_me, inputs=[topic_q, lvl_q, n_q], outputs=out4)\n",
    "    return demo\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting AI Tutor MCP Toolkit on port 7860â€¦\")\n",
    "    build_demo().launch(server_name = \"0.0.0.0\", mcp_server = True)\n",
    "# --- END OF FILE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
